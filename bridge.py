#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Autonomous Venture Intelligence Analyst
–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ò–ò-—Ä–µ–¥–∞–∫—Ç–æ—Ä –≤–µ–Ω—á—É—Ä–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞
"""

import os
import requests
import re
import feedparser
import random
from datetime import datetime, timedelta
import google.generativeai as genai
from supabase import create_client
from bs4 import BeautifulSoup
import time

# ============================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
TELEGRAM_ADMIN_ID = os.getenv("TELEGRAM_ADMIN_ID")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY")
GOOGLE_SEARCH_API_KEY = os.getenv("GOOGLE_SEARCH_API_KEY")
GOOGLE_SEARCH_ENGINE_ID = os.getenv("GOOGLE_SEARCH_ENGINE_ID")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
client = genai.Client(api_key=GEMINI_API_KEY)
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ============================================
# –ò–°–¢–û–ß–ù–ò–ö–ò –ù–û–í–û–°–¢–ï–ô
# ============================================

RSS_FEEDS = [
    # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –ê–∑–∏—è
    "https://kursiv.kz/feed/",
    "https://kursiv.kz/news/StartUp/feed/",
    "https://digitalbusiness.kz/feed/",
    "https://digitalbusiness.kz/category/startups/feed/",
    "https://forbes.kz/rss",
    "https://spot.uz/ru/feed/",
    "https://capital.kz/rss",
    "https://bluescreen.kz/feed/",
    "https://weproject.media/feed/",
    # Tier-1 Global
    "https://techcrunch.com/feed/",
    "https://news.crunchbase.com/feed/",
    "https://www.ycombinator.com/blog/feed",
]

SCRAPE_SITES = [
    {"name": "IT Park Uzbekistan", "url": "https://it-park.uz/ru/news", "selector": "article, .news-item", "region": "UZ"},
    {"name": "Astana Hub", "url": "https://astanahub.com/ru/news/", "selector": "article", "region": "KZ"},
]

KEYWORDS = [
    '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '—Å—Ç–∞—Ä—Ç–∞–ø', '–≤–µ–Ω—á—É—Ä', '—Ñ–æ–Ω–¥', '—Ä–∞—É–Ω–¥', '–ø—Ä–∏–≤–ª–µ–∫',
    '–∑–∞–ø—É—Å–∫', '–º–ª–Ω', '–º–∏–ª–ª–∏–æ–Ω', 'seed', 'exit', '–≤—ã—Ö–æ–¥', '—Å–¥–µ–ª–∫–∞',
    '—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∫–∞–ø–∏—Ç–∞–ª', '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', '–∏–Ω–∫—É–±–∞—Ç–æ—Ä',
    'investment', 'startup', 'venture', 'fund', 'series', 'round',
    'raised', 'million', 'funding', 'capital', 'accelerator', 'exit',
    'MA7', 'Tumar', 'White Hill', 'Big Sky', 'Most Ventures',
    'Axiom Capital', 'Jas Ventures', 'a16z', 'Sequoia', 'YC'
]

TIER_1_ENTITIES = [
    'a16z', 'Andreessen Horowitz', 'Sequoia', 'Y Combinator', 'YC',
    'OpenAI', 'Anthropic', 'Google Ventures', 'Accel'
]

LOCAL_ENTITIES = [
    'MA7 Ventures', 'Tumar Ventures', 'White Hill Capital', 'Big Sky Ventures',
    'Most Ventures', 'Axiom Capital', 'Jas Ventures', 'Astana Hub',
    'Kaspi', 'Chocofamily', 'Kolesa', 'Arbuz.kz'
]

# ============================================
# –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ù–¢ (ACTIVAT VC)
# ============================================

ACTIVAT_VC_TOPICS = [
    {
        "topic": "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π",
        "content": "–¢—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å (% –≥–æ–¥–æ–≤—ã—Ö), –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏), –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å (—Å–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂–∏). –î–µ–ø–æ–∑–∏—Ç: 12%, 100%, 33%. –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å: 15-22%, 70%, 1%. –í–µ–Ω—á—É—Ä: –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ, 0%, –Ω–∏–∑–∫–∞—è."
    },
    {
        "topic": "–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ç–∞—Ä—Ç–∞–ø–∞",
        "content": "5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: —é–Ω–æ—Å—Ç—å (3-5 –ª–µ—Ç), –Ω–æ–≤–∏–∑–Ω–∞ –∏–¥–µ–∏, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–Ω–æ—Å—Ç—å (IT), –±–æ–ª—å—à–æ–π —Ä—ã–Ω–æ–∫. Uber ‚Äî –Ω–µ —Ç–∞–∫—Å–æ–ø–∞—Ä–∫, –∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ."
    },
    {
        "topic": "–í–µ–Ω—á—É—Ä–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
        "content": "Venture = —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–π. –í—ã—Å–æ–∫–∞—è —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç—å, –Ω–æ –æ–¥–∏–Ω —É—Å–ø–µ—à–Ω—ã–π –æ–∫—É–ø–∞–µ—Ç 10. –ò–∫—Å—ã: 5x, 10x, 100x. –ü—Ä–∞–≤–∏–ª–æ: 5-10% –ø–æ—Ä—Ç—Ñ–µ–ª—è. –ü—É—Ç—å: —á–∞—Å—Ç–Ω—ã–π –∏–Ω–≤–µ—Å—Ç–æ—Ä ‚Üí –±–∏–∑–Ω–µ—Å-–∞–Ω–≥–µ–ª ‚Üí —Å—É–ø–µ—Ä-–∞–Ω–≥–µ–ª ‚Üí —Ñ–æ–Ω–¥."
    },
    {
        "topic": "–ì–¥–µ –∏—Å–∫–∞—Ç—å —Å—Ç–∞—Ä—Ç–∞–ø—ã",
        "content": "4 –∫–∞–Ω–∞–ª–∞: Google, LinkedIn/Facebook, Demo Days, –Ω–µ—Ç–≤–æ—Ä–∫–∏–Ω–≥. –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ = —Å–≤–æ–±–æ–¥–∞, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –í –∫–ª—É–±–µ = –±–æ–ª—å—à–µ —Å–¥–µ–ª–æ–∫, —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞."
    },
    {
        "topic": "–í–æ–ø—Ä–æ—Å—ã –ø–µ—Ä–µ–¥ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–µ–π",
        "content": "–û–±—Å—É–¥–∏—Ç—å: —Å—É–º–º—É –∏ –≤–∞–ª—é—Ç—É, —Å—Ä–æ–∫–∏, —É—Å–ª–æ–≤–∏—è —Ç—Ä–∞–Ω—à–µ–π (KPI), –¥–æ–ª—é (–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è/–æ—Ü–µ–Ω–∫–∞), –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç—å. SAFE, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º—ã–µ –∑–∞–π–º—ã."
    },
    {
        "topic": "–§–æ—Ä–º—ã –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π",
        "content": "–ü—Ä—è–º—ã–µ (cash-in/out), –≤–µ–Ω—á—É—Ä–Ω—ã–µ —Ñ–æ–Ω–¥—ã, —á–∞—Å—Ç–Ω—ã–µ, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ, –∫—Ä–∞—É–¥—Ñ–∞–Ω–¥–∏–Ω–≥ (lending/equity)."
    },
    {
        "topic": "–ü—Ä–æ—Ü–µ—Å—Å –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
        "content": "7 —ç—Ç–∞–ø–æ–≤: —Ü–µ–ª–∏, –ø–æ–∏—Å–∫, Due Diligence (—Ñ–∏–Ω–∞–Ω—Å—ã, —é—Ä., –º–∞—Ä–∫–µ—Ç–∏–Ω–≥), –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã, Term Sheet, –ø–µ—Ä–µ–¥–∞—á–∞, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥."
    },
    {
        "topic": "–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤—ã—Ö–æ–¥–∞",
        "content": "5 —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤: —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞—É–Ω–¥, –ø–æ–≥–ª–æ—â–µ–Ω–∏–µ (M&A), –æ–ø—Ü–∏–æ–Ω (–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å –≤—ã–∫—É–ø–∞–µ—Ç), –ø—Ä–æ–¥–∞–∂–∞ —Å—Ç–æ—Ä–æ–Ω–Ω–µ–º—É, –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ. –û–±–≥–æ–≤–æ—Ä–∏—Ç—å –î–û –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π!"
    }
]

# ============================================
# –ü–†–û–ú–ü–¢–´
# ============================================

SYSTEM_NEWS = """–¢—ã ‚Äî Senior Venture Analyst, —Ä–µ–¥–∞–∫—Ç–æ—Ä —ç–ª–∏—Ç–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –æ –≤–µ–Ω—á—É—Ä–Ω–æ–º —Ä—ã–Ω–∫–µ –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π –ê–∑–∏–∏.

–§–û–†–ú–ê–¢ (10-15 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π):

[–≠–º–æ–¥–∑–∏] –ó–∞–≥–æ–ª–æ–≤–æ–∫

–ß–¢–û –ü–†–û–ò–ó–û–®–õ–û (2-3):
–°—É—Ç—å, —Ü–∏—Ñ—Ä—ã, —É—á–∞—Å—Ç–Ω–∏–∫–∏

–ö–¢–û –í–û–í–õ–ï–ß–Å–ù (2-3):
–û –∫–æ–º–ø–∞–Ω–∏–∏/—Ñ–æ–Ω–¥–µ, —Å—Ç–∞–¥–∏—è

–ü–û–ß–ï–ú–£ –í–ê–ñ–ù–û (3-5):
–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä—ã–Ω–∫–∞, —Ç—Ä–µ–Ω–¥—ã, –≤–ª–∏—è–Ω–∏–µ

–í–´–í–û–î–´ (1-2):
–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç

–î–∞—Ç–∞: [–¥–∞—Ç–∞]
–ò—Å—Ç–æ—á–Ω–∏–∫: [–Ω–∞–∑–≤–∞–Ω–∏–µ]
–°—Å—ã–ª–∫–∞: [URL]

–ë–ï–ó markdown. –ò–∑–±–µ–≥–∞–π –∫–ª–∏—à–µ. –ö–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç, –Ω–µ –ò–ò."""

SYSTEM_EDU = """–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç (5-8 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π).
–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–ª—å–∑–∞. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –ë–ï–ó markdown."""

# ============================================
# –§–£–ù–ö–¶–ò–ò
# ============================================

def send_admin(text):
    if not TELEGRAM_ADMIN_ID:
        print(f"‚ö†Ô∏è {text}")
        return
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    requests.post(url, data={"chat_id": TELEGRAM_ADMIN_ID, "text": f"ü§ñ {text}"}, timeout=10)

def google_search(query):
    if not GOOGLE_SEARCH_API_KEY:
        return []
    try:
        url = "https://www.googleapis.com/customsearch/v1"
        params = {"key": GOOGLE_SEARCH_API_KEY, "cx": GOOGLE_SEARCH_ENGINE_ID, "q": query, "num": 3}
        r = requests.get(url, params=params, timeout=10)
        if r.status_code == 200:
            return [{"title": i["title"], "link": i["link"]} for i in r.json().get("items", [])]
    except:
        pass
    return []

def find_source(text, url):
    for e in LOCAL_ENTITIES:
        if e.lower() in text.lower():
            results = google_search(f"{e} official website")
            if results:
                site = results[0]['link']
                try:
                    supabase.table("tracked_entities").upsert({
                        "entity_name": e,
                        "entity_type": "fund" if "ventures" in e.lower() else "startup",
                        "website": site
                    }, on_conflict="entity_name").execute()
                except:
                    pass
                return site
    return url

def calc_share(news):
    score = 5
    text = (news.get('title', '') + ' ' + news.get('summary', '')).lower()
    if any(w in text for w in ['–º–ª–Ω', 'million', '$', '‚Ç∏']):
        score += 2
    if any(e.lower() in text for e in TIER_1_ENTITIES + LOCAL_ENTITIES):
        score += 2
    if any(w in text for w in ['seed', 'series', '—Ä–∞—É–Ω–¥']):
        score += 1
    if any(w in text for w in ['–º–æ–∂–µ—Ç', '–ø–ª–∞–Ω–∏—Ä—É–µ—Ç']):
        score -= 2
    if not re.search(r'\d', text):
        score -= 3
    return max(1, min(10, score))

def get_activat():
    topic = random.choice(ACTIVAT_VC_TOPICS)
    return f"""–ü–æ—Å—Ç –∏–∑ Activat VC:

–¢–µ–º–∞: {topic['topic']}
–ú–∞—Ç–µ—Ä–∏–∞–ª: {topic['content']}

5-8 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –ø—Ä–∏–º–µ—Ä—ã.
–í –∫–æ–Ω—Ü–µ: "–ò—Å—Ç–æ—á–Ω–∏–∫: –ö—É—Ä—Å Activat VC"
"""

def parse_rss():
    print("üì° RSS...")
    fresh = []
    day_ago = datetime.now() - timedelta(days=1)
    
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:20]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')
                link = entry.get('link', '')
                
                pub = entry.get('published_parsed') or entry.get('updated_parsed')
                if pub:
                    pub_date = datetime(*pub[:6])
                    if pub_date < day_ago:
                        continue
                else:
                    pub_date = datetime.now()
                
                text = (title + ' ' + summary).lower()
                if any(kw.lower() in text for kw in KEYWORDS):
                    item = {
                        'title': title,
                        'summary': BeautifulSoup(summary, 'html.parser').get_text()[:800],
                        'link': link,
                        'date': pub_date.strftime('%d.%m.%Y'),
                        'source': feed_url.split('/')[2],
                        'is_tier1': any(t.lower() in text for t in TIER_1_ENTITIES)
                    }
                    item['shareability'] = calc_share(item)
                    fresh.append(item)
        except:
            pass
        time.sleep(0.5)
    
    if fresh:
        fresh.sort(key=lambda x: (x['shareability'], datetime.strptime(x['date'], '%d.%m.%Y')), reverse=True)
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(fresh)}")
    return fresh

def scrape_site(site):
    try:
        r = requests.get(site['url'], headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        if r.status_code != 200:
            return []
        soup = BeautifulSoup(r.content, 'html.parser')
        articles = soup.select(site['selector'])[:10]
        news = []
        for a in articles:
            try:
                title_tag = a.find(['h1', 'h2', 'h3', 'a'])
                if not title_tag:
                    continue
                title = title_tag.get_text(strip=True)
                link_tag = a.find('a')
                href = link_tag.get('href', '') if link_tag else ''
                if href and not href.startswith('http'):
                    href = site['url'].rstrip('/') + '/' + href.lstrip('/')
                if any(kw.lower() in title.lower() for kw in KEYWORDS):
                    item = {
                        'title': title, 'summary': '', 'link': href,
                        'date': datetime.now().strftime('%d.%m.%Y'),
                        'source': site['name'],
                        'is_tier1': site.get('region') == 'GLOBAL'
                    }
                    item['shareability'] = calc_share(item)
                    news.append(item)
            except:
                continue
        return news
    except:
        return []

def extract_img(url):
    if not url or not url.startswith('http'):
        return None
    try:
        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        if r.status_code != 200:
            return None
        soup = BeautifulSoup(r.content, 'html.parser')
        
        for prop in ['og:image', 'twitter:image']:
            meta = soup.find('meta', property=prop) or soup.find('meta', attrs={'name': prop})
            if meta and meta.get('content'):
                img = meta['content']
                if img.startswith('http'):
                    return img
        return None
    except:
        return None

def unsplash(kw="startup"):
    if not UNSPLASH_ACCESS_KEY:
        return None
    try:
        url = f"https://api.unsplash.com/photos/random?query={kw}&client_id={UNSPLASH_ACCESS_KEY}&orientation=landscape"
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            return r.json()['urls']['regular']
    except:
        pass
    return None

# ============================================
# MAIN
# ============================================

def main():
    try:
        print("="*60)
        print("üöÄ –ó–ê–ü–£–°–ö")
        print("="*60)
        send_admin("üöÄ –ó–∞–ø—É—Å–∫")
        
        # –ê–Ω—Ç–∏-–∫–µ–π—Å—ã
        try:
            neg = supabase.table("negative_constraints").select("feedback").execute()
            neg_ctx = "\n".join([f["feedback"] for f in neg.data]) if neg.data else ""
        except:
            neg_ctx = ""
        
        # –†–µ–∂–∏–º
        hour = datetime.now().hour
        print(f"üïê UTC: {datetime.now().strftime('%H:%M')}")
        
        if hour == 3:
            mode = "–ù–û–í–û–°–¢–¨"
        elif hour == 12:
            mode = "–û–ë–£–ß–ï–ù–ò–ï"
        else:
            mode = "–ù–û–í–û–°–¢–¨" if hour < 12 else "–û–ë–£–ß–ï–ù–ò–ï"
        
        print(f"üìã –†–µ–∂–∏–º: {mode}")
        
        img = None
        fresh = []
        
        if mode == "–ù–û–í–û–°–¢–¨":
            fresh = parse_rss()
            
            if len(fresh) < 5:
                print("üï∑Ô∏è –°–∫—Ä–µ–π–ø–∏–Ω–≥...")
                for site in SCRAPE_SITES:
                    fresh.extend(scrape_site(site))
                    time.sleep(1)
            
            fresh = [n for n in fresh if n.get('shareability', 0) >= 6]
            print(f"üéØ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞: {len(fresh)}")
            
            if fresh:
                news = fresh[0]
                print(f"\n‚úÖ –í–´–ë–†–ê–ù–ê:\n   {news['title']}")
                
                orig = find_source(news['title'] + ' ' + news['summary'], news['link'])
                
                prompt = f"""–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç (10-15 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π):

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title']}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {news['summary']}
–°—Å—ã–ª–∫–∞: {orig}
–î–∞—Ç–∞: {news['date']}
–ò—Å—Ç–æ—á–Ω–∏–∫: {news['source']}

–ê–Ω—Ç–∏-–∫–µ–π—Å—ã: {neg_ctx if neg_ctx else "–ù–µ—Ç"}

–°–¢–†–£–ö–¢–£–†–ê:
–ß–¢–û –ü–†–û–ò–ó–û–®–õ–û (2-3)
–ö–¢–û –í–û–í–õ–ï–ß–Å–ù (2-3)
–ü–û–ß–ï–ú–£ –í–ê–ñ–ù–û (3-5)
–í–´–í–û–î–´ (1-2)

–î–∞—Ç–∞: {news['date']}
–ò—Å—Ç–æ—á–Ω–∏–∫: {news['source']}
–°—Å—ã–ª–∫–∞: {orig}
"""
                img = extract_img(news['link']) or unsplash("venture capital")
            else:
                mode = "–û–ë–£–ß–ï–ù–ò–ï"
        
        if mode == "–û–ë–£–ß–ï–ù–ò–ï":
            use_act = random.choice([True, False])
            if use_act:
                prompt = get_activat()
            else:
                prompt = "–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç (5-8 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π): –°—Ç–∞–¥–∏–∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π / –ú–µ—Ç—Ä–∏–∫–∏ / Unit Economics / Term Sheet / Due Diligence. –° –ø—Ä–∏–º–µ—Ä–∞–º–∏."
            img = unsplash("business education")
        
        # Gemini
        print("ü§ñ Gemini...")
        sys = SYSTEM_NEWS if mode == "–ù–û–í–û–°–¢–¨" else SYSTEM_EDU
        
        resp = client.models.generate_content(
            model="gemini-2.5-flash",
            config={"system_instruction": sys, "temperature": 0.7},
            contents=prompt
        )
        
        if not resp or not resp.text:
            raise Exception("Gemini –ø—É—Å—Ç–æ")
        
        text = resp.text.strip().replace('**', '').replace('__', '').replace('*', '').replace('_', '')
        
        forbidden = ["—Å–∏–º—É–ª—è—Ü–∏", "—è –∏—â—É", "Visual Prompt"]
        if any(w.lower() in text.lower() for w in forbidden):
            send_admin("‚ö†Ô∏è –ó–∞–ø—Ä–µ—â—ë–Ω–Ω–æ–µ —Å–ª–æ–≤–æ")
            return
        
        while "---" in text:
            text = text.split("---", 1)[0].strip()
        
        print(f"‚úÖ –ì–æ—Ç–æ–≤–æ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        if mode == "–ù–û–í–û–°–¢–¨" and fresh:
            dedup = fresh[0]['link']
            share = fresh[0].get('shareability', 0)
            src_type = "tier1" if fresh[0].get('is_tier1') else "local"
        else:
            dedup = text[:100].strip().replace('\n', ' ')
            share = 0
            src_type = "education"
        
        check = supabase.table("posted_news").select("url_text").eq("url_text", dedup).execute()
        if check.data:
            print("‚ùå –î—É–±–ª–∏–∫–∞—Ç")
            send_admin("‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç")
            return
        
        # –ü—É–±–ª–∏–∫–∞—Ü–∏—è
        print("üì§ –ü—É–±–ª–∏–∫–∞—Ü–∏—è...")
        msg = text if len(text) <= 4000 else text[:3997] + "..."
        
        if img:
            url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"
            payload = {"chat_id": TELEGRAM_CHAT_ID, "photo": img, "caption": msg}
        else:
            url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
            payload = {"chat_id": TELEGRAM_CHAT_ID, "text": msg}
        
        r = requests.post(url, data=payload, timeout=30)
        
        if r.status_code == 200:
            supabase.table("posted_news").insert({
                "url_text": dedup,
                "news_type": mode,
                "shareability_score": share,
                "source_type": src_type
            }).execute()
            
            print("üéâ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û!")
            
            preview = text[:100] + "..."
            img_st = "‚úÖ –∫–∞—Ä—Ç–∏–Ω–∫–∞" if (img and "unsplash" not in str(img).lower()) else "üé® Unsplash" if img else "–±–µ–∑"
            send_admin(f"‚úÖ {mode}, {img_st}:\n\n{preview}")
        else:
            raise Exception(f"Telegram {r.status_code}")
    
    except Exception as e:
        print(f"\nüí• {e}")
        send_admin(f"üí• {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
